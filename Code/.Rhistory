library(faraway)
require(faraway)
install.packages("faraway")
library(faraway)
help(faraway)
library(faraway)
help(faraway)
?faraway
??faraway
library(faraway)
data("sat",faraway)
library(faraway)
data(sat,package = "faraway"")
library(faraway)
data(sat,package = "faraway")
summary(sat)
library(ggplot2)
lmod <- lm(formula= total ~ expend + ratio + salary + takers, data = sat)
plot_residuals(lmod)
install.packages("broom")
library(broom)
plot_residuals(lmod)
?broom
library(faraway)
library(ggplot2)
library(broom)
data(sat,package = "faraway")
lmod <- lm(formula= total ~ expend + ratio + salary + takers, data = sat)
#Function taken from link given below
plot_residuals <- function(model, size = 1, linewidth = 1){
# extract residuals
require(broom)
d <- broom::augment(model)
# 1: residuals vs. fitted
require(ggplot2)
ggplot(data = d, aes(x = .fitted, y = .resid)) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 0, linetype = 2) -> p1
## 2: qq-plot
probs <- c(0.25, 0.75)
y <- quantile(d$.std.resid, probs, names = FALSE, na.rm = TRUE)
x <- qnorm(probs)
slope <- diff(y)/diff(x)
intercept <- y[1L] - slope * x[1L]
ggplot(data = d, aes(sample = .std.resid)) +
geom_qq(size = size) +
geom_abline(intercept = intercept, slope = slope,
linetype = 2, size = linewidth) -> p2
## 3: scale location plot
ggplot(data = d, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 1) -> p3
## 4: Cooks distance plot
ggplot(data = d, aes(.hat, .std.resid)) +
geom_vline(size = linewidth, xintercept = 0) +
geom_hline(size = linewidth, yintercept = 0) +
geom_point(aes(size = .cooksd)) +
geom_smooth(se = FALSE) -> p4
require(gridExtra)
return(plot(arrangeGrob(p1, p2, p3, p4, nrow = 2)))
}
plot_residuals(lmod)
marginalModelPlots(lmod)
install.packages("car")
library(car)
marginalModelPlots(lmod)
?marginalModelPlots
?Halfnorm
?halfnorm
library(faraway)
library(ggplot2)
library(broom)
data(sat,package = "faraway")
lmod <- lm(formula= total ~ expend + ratio + salary + takers, data = sat)
hatv <- hatvalues(lmod)
halfnorm(hatv, ylab = "Leverage")
#Function taken from link given below
plot_residuals <- function(model, size = 1, linewidth = 1){
# extract residuals
require(broom)
d <- broom::augment(model)
# 1: residuals vs. fitted
require(ggplot2)
ggplot(data = d, aes(x = .fitted, y = .resid)) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 0, linetype = 2) -> p1
## 2: qq-plot
probs <- c(0.25, 0.75)
y <- quantile(d$.std.resid, probs, names = FALSE, na.rm = TRUE)
x <- qnorm(probs)
slope <- diff(y)/diff(x)
intercept <- y[1L] - slope * x[1L]
ggplot(data = d, aes(sample = .std.resid)) +
geom_qq(size = size) +
geom_abline(intercept = intercept, slope = slope,
linetype = 2, size = linewidth) -> p2
## 3: scale location plot
ggplot(data = d, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 1) -> p3
## 4: Cooks distance plot
ggplot(data = d, aes(.hat, .std.resid)) +
geom_vline(size = linewidth, xintercept = 0) +
geom_hline(size = linewidth, yintercept = 0) +
geom_point(aes(size = .cooksd)) +
geom_smooth(se = FALSE) -> p4
require(gridExtra)
return(plot(arrangeGrob(p1, p2, p3, p4, nrow = 2)))
}
plot_residuals(lmod)
### If you do not have the BiocStyle package, for the formatting  you need
### to install it with the following code:
if("BiocStyle" %in% rownames(installed.packages()) == FALSE)
{source("https://bioconductor.org/biocLite.R")
biocLite("BiocStyle")}
library(faraway)
library(ggplot2)
library(broom)
data(sat,package = "faraway")
lmod <- lm(formula= total ~ expend + ratio + salary + takers, data = sat)
#Plot leverages vs half normal quantiles (detect outlying leverages)
hatv <- hatvalues(lmod)
halfnorm(hatv, ylab = "Leverage")
#This function is taken from link provided in the problem set
plot_residuals <- function(model, size = 1, linewidth = 1){
# extract residuals
require(broom)
d <- broom::augment(model)
# 1: residuals vs. fitted
require(ggplot2)
ggplot(data = d, aes(x = .fitted, y = .resid)) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 0, linetype = 2) -> p1
## 2: qq-plot
probs <- c(0.25, 0.75)
y <- quantile(d$.std.resid, probs, names = FALSE, na.rm = TRUE)
x <- qnorm(probs)
slope <- diff(y)/diff(x)
intercept <- y[1L] - slope * x[1L]
ggplot(data = d, aes(sample = .std.resid)) +
geom_qq(size = size) +
geom_abline(intercept = intercept, slope = slope,
linetype = 2, size = linewidth) -> p2
## 3: scale location plot
ggplot(data = d, aes(x = .fitted, y = sqrt(abs(.std.resid)))) +
geom_point(size = size) +
geom_smooth() +
geom_hline(yintercept = 1) -> p3
## 4: Cooks distance plot
ggplot(data = d, aes(.hat, .std.resid)) +
geom_vline(size = linewidth, xintercept = 0) +
geom_hline(size = linewidth, yintercept = 0) +
geom_point(aes(size = .cooksd)) +
geom_smooth(se = FALSE) -> p4
require(gridExtra)
return(plot(arrangeGrob(p1, p2, p3, p4, nrow = 2)))
}
plot_residuals(lmod)
#Marginal model plots
library(car)
marginalModelPlots(lmod)
install.packages("gghalfnorm")
library(gghalfnorm)
gghalfnorm(hatv, ylab = "Leverage")
help("gghalfnorm")
gghalfnorm(hatv) + labs(title = "Leverages vs Half-normal quantiles", y = "Leverage")
marginalModelPlots(lmod)
knitr::opts_chunk$set(echo = TRUE)
#simple text processing functions
truncate_text = function(raw_text, max_length) {
# truncate tail of raw_text to have max_length words (at most)
paste(unlist(strsplit(raw_text, " "))[1:max_length], sep = " ", collapse = " ")
}
sample_words = function(text, n) {
#returns n words from text by randomly sample them without replacements
paste(sample(unlist(strsplit(text, " ")), n, replace = FALSE), sep = " ", collapse = " ")
}
num_of_words = function(raw_text) {
#count number of words in raw_text
length(unlist(strsplit(raw_text, " ")))
}
count_words = function(raw_text,
ignore_list = c(""),
min.counts = 0) {
# return the number of appearance of each word in raw_text
# pre-processing removes punctuation and words from the ignore_list
words <- tolower(raw_text)
words <- gsub("[[:punct:]]", " ", words)
words <- gsub('[0-9]+', " ", words)
words <- unlist(strsplit(words, " "))
words <- words[!words == ""]
words <- words[!words == " "]
words <- words[!words == "\n"]
#  words <- wordStem(words)
words <- words[!is.element(words, ignore_list)]
tble <- sort(table(words))
df <- as.data.frame(tble)
df <- filter(df, Freq >= min.counts)
df
}
reduce_counts = function(counts, word_list) {
# reduce the table of counts to words in word_list only
# (returns a list of counts_
c <- rep(0, length(word_list))
#df <- counts[counts$words %in% word_list,]
for(i in seq_along(word_list)) {
fr = counts$Freq[counts$words == word_list[i]]
if (length(fr) > 0) {
c[i] <- fr
}
}
c
}
get_word_frequency = function(text, words) {
#get frequency of usage of words in list 'words'
c0 = word_count(text)
freq = c0 / sum(c0)
ferq_common_words = reduce_counts(freq, words)
}
names(data)
library(readr)
library(dplyr)
library(tidyverse)
library(data.table)
library(SnowballC)
install.packages("SnowballC")
library(readr)
library(dplyr)
library(tidyverse)
library(data.table)
library(SnowballC)
setwd("~/Desktop/Fall_2018/Stats_285/Stats235-Hackathon-Tukey/Code")
source("./HC_aux.R") #load functions for computing HC
data <- read.csv("../Data/speech_w_data_example.csv", sep = "|")
names(data)
data %>%
filter(phrasecount > 9, party == D, phrase)
data
names(data)
data %>%
filter(phrasecount > 9, party == D, phrase)
names(data)
#feature exrtraction using all Ham and Mad texts
# this list contains "contextual" words that are not useful for distinguishin between the authors
ignore_list = c('govern', 'trade',
'enemi','govern', 'legislatur','xii', 'z',
'x', 'v', 'xi', 'xiii', 'vi', 'vii', 'ii',
'iii', 'ix', 'war','senat', 'constitut', 'treati',
'government', 'senate', 'president', 'jurisdiction',
'treaty', 'legislature', 'france', 'state', 's', 'laws','union', 'national', 'power',
'constitution','constitution', 'security','nations', 'british', 'states',
'commerce', 'courts', 'enemies', 'britain', 'citizens','citizen','public', 'system','treaties')
corpus1 = ham.all
?readr
?SnowballC
??SnowballC
library(SnowballC)
?SnowballC
knitr::opts_chunk$set(echo = TRUE)
#load speech data
library(readr)
setwd("~/Desktop/Fall_2018/Stats_285/Stats235-Hackathon-Tukey/Code")
raw.corpus <- read_csv("../Data/speech_w_data_example.csv")
#select two units from raw corpus for comparison
library(tidyverse)
unit1 <- raw.corpus %>%
filter(party == 'R', chamber == 'H', congress_id == 114) %>%   # (R)ebulican party (H)ouse speeches from 114th congress
select(speech_id, speech)
unit2 <- raw.corpus %>%
filter(party == 'D', chamber == 'H', congress_id == 114) %>%  # (D)emocratic party (H)ouse speeches from 114th congress
select(speech_id, speech)
#list of words to ignore
words_to_ignore = c('mr', 'unit','unanim', 'don', 'ask', 'presid','madam', 'american', 'make', 'nation',
'let','last', 'urg', 'year', 'new', 'didn', 'dosen', 'bil', 'print', 'speaker', 'men',
'side', 'yield', 'rank', 'rise', 'small', 'yet', 'yesterday', 'move', 'york', 'long',
'yes', 'xii', 'go', 'billion', 'republican', 'democrat', 'congress', 'iv','iiv', 'doesnt', 'cant',
'cannot', 'pelosi','weve','im','theyr', 'didnt', 'he', 'you', 'senat', 'hous', 'she',
'dont', 'got')
#other lists:
source("./word_lists.R") #file containing list of words
ignore_list = c(words_to_ignore, singletons, additional_words1, additional_words2, function_words)
# test differences between two test unit
source("./two_unit_test.R") #file containing test routine
HC <- two_unit_test(unit1, unit2, ignore_list)
#select two units from raw corpus for comparison
library(tidyverse)
unit1 <- raw.corpus %>%
filter(party == 'R', chamber == 'H', congress_id == 114) %>%   # (R)ebulican party (H)ouse speeches from 114th congress
select(speech_id, speech)
unit2 <- raw.corpus %>%
filter(party == 'D', chamber == 'H', congress_id == 114) %>%  # (D)emocratic party (H)ouse speeches from 114th congress
select(speech_id, speech)
# test differences between two test unit
source("./two_unit_test.R") #file containing test routine
HC <- two_unit_test(unit1, unit2, ignore_list)
install.packages("tidytext")
# test differences between two test unit
source("./two_unit_test.R") #file containing test routine
HC <- two_unit_test(unit1, unit2, ignore_list)
max(HC$zz)
#67.1053824015321  suggest high difference in words used in speeches by each party
#which words cause the difference ?
HC %>%
top_n(-30, pp) %>%
ggplot(aes(reorder(word, pp), pp)) +
geom_col(colour = 'red') +
ylab('p-val') +
xlab('word') +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ggtitle("words associated with lowest p-vals")
i.max.star <- which.max(HC$zz)
if (i.max.star == 1) {  #if i.max.star == 1 then remove this entry and compute again
i.max.star <- which.max(HC$zz)
i.max.star <- which.max(HC$zz[2:length(HC$zz)])+1
}
hc.star = HC$zz[i.max.star]
uu = HC$uu
zz = HC$zz
#show z scores of p-values used for HC
ggplot() +
geom_point(data = HC, aes(uu, zz), colour = 'red', size = 1) +
geom_vline(xintercept  = i.max.star/length(uu), colour = 'red') +
geom_point(data = HC, aes(uu, zz), colour = 'blue', size = 1)+
geom_vline(xintercept = i.max.star/length(uu), colour = 'blue') +
geom_segment(x=0, y = hc.star, xend = i.max.star/length(uu), yend = hc.star, color = 'blue') +
ggtitle("Z-scores (p-vals)")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
raw.corpus <- read_csv("../Data//speech_w_data_example.csv")
library(tidyverse)
# compare word usage between the (H)ouse and the (S)enate in the 114th congress
unit1 <- raw.corpus %>%
filter(party == 'R', chamber == 'H', congress_id == 114) %>%
select(speech_id, speech)
unit2 <- raw.corpus %>%
filter(party == 'D', chamber == 'H', congress_id == 114) %>%
select(speech_id, speech)
library(tidytext)
library(SnowballC) #for stemming
#compute tf-idf of each corpus (document = speeche)
unit1.dt <- unit1 %>%
unnest_tokens(word, speech) %>%
mutate(word = wordStem(str_extract(word, "[a-z']+"))) %>%
count(speech_id, word, sort = TRUE) %>%
bind_tf_idf(word, speech_id, n)
unit2.dt <- unit2 %>%
unnest_tokens(word, speech) %>%
mutate(word = wordStem(str_extract(word, "[a-z']+"))) %>%
count(speech_id, word, sort = TRUE) %>%
bind_tf_idf(word, speech_id, n)
#for each topic consider only k most dominant words
k =
counts1 <- unit1.dt %>%
arrange(desc(tf_idf)) %>%
#filter(speech_id < 1110000050) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(speech_id) %>%
top_n(k) %>%
ungroup %>%
count(word, sort = TRUE)
#for each topic consider only k most dominant words
k = 10
counts1 <- unit1.dt %>%
arrange(desc(tf_idf)) %>%
#filter(speech_id < 1110000050) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(speech_id) %>%
top_n(k) %>%
ungroup %>%
count(word, sort = TRUE)
counts2 <- unit2.dt %>%
arrange(desc(tf_idf)) %>%
#filter(speech_id < 1110000050) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(speech_id) %>%
top_n(k) %>%
ungroup %>%
count(word, sort = TRUE)
source("./HC_aux.R")
two.counts <- counts1 %>%
inner_join(counts2, by = 'word') %>%
mutate(total.x = sum(nn.x), total.y = sum(nn.y)) %>%
mutate(total = nn.x + nn.y) %>%
filter(total > 10) %>%
rowwise() %>%
mutate(p = (nn.x+nn.y) / (total.x + total.y)) %>%
mutate(se = sqrt(p*(1-p)*(1/total.x + 1/ total.y))) %>%
mutate(z.score = (nn.x /total.x - nn.y / total.y) / se) %>%
mutate(pval = 2*pnorm(-abs(z.score))) %>%
dplyr::select(word, nn.x, nn.y, pval, total.x, total.y)
hc = hc.vals(two.counts$pval, alpha = 0.5)
hc$hc.star
HC <- data_frame(uu = hc$uu, zz = hc$z, pp = hc$p.sorted, word = two.counts$word[hc$p.sorted_idx])
two.counts %>%
arrange(desc(pval)) %>%
top_n(-20)
#lowest p-values
#HC %>% arrange(desc(pp)) %>% top_n(-20, pp)
HC %>%
top_n(-20, pp) %>%
ggplot(aes(reorder(word, pp), pp)) +
geom_col(colour = 'red') +
ylab('p-val') +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ggtitle("lowest p-vals")
