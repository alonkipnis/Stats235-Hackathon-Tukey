---
title: "HC_2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(tm)

library(tidyverse)
library(data.table)

#setwd("./Stats235 Hackathon//Code")
source("./HC_aux.R") #load functions for computing HC
```


```{r}
raw.corpus <- read_csv("~/Downloads/CongRec/speech_w_data.csv")
```


```{r}
unit1 <- raw.corpus %>%
    filter(party == 'R', chamber == 'H', congress_id == 111) %>%
    select(speech_id, speech)
```


```{r}
#separate speech into n-grams
library(tidytext)
library(SnowballC)

unit1.words <- unit1 %>%
    unnest_tokens(word, speech, token = "words") %>%
    mutate(word = str_extract(word, "[a-z']+")) %>%
    drop_na %>% 
    mutate(word = wordStem(word)) %>%
    count(word, speech_id, sort = TRUE) %>%
    mutate(total = sum(n))
```

```{r}
# comput p-vals and HC
source("./word_lists.R") #file containing list of words
ignore_list = c(singletons, additional_words1, additional_words2, additional_words3, function_words)

word.counts <- unit1.words %>% 
    inner_join(unit2.words, by = 'word') %>%
    filter(!(word %in% c(ignore_list, function_words, additional_words1, additional_words2, singletons))) %>%
    mutate(total = total.x + total.y) %>%
    filter(total > 15) %>%
    rowwise() %>%
        mutate(p = (n.x+n.y) / (total.x + total.y)) %>%
        mutate(se = sqrt(p*(1-p)*(1/total.x + 1/ total.y))) %>%
        mutate(z.score = (n.x /total.x - n.y / total.y) / se) %>%
        mutate(pval = 2*pnorm(-abs(z.score))) %>%
    dplyr::select(word, n.x, n.y, pval) 
    
hc = hc.vals(word.counts$pval, alpha = 0.5)
hc$hc.star
HC <- data_frame(uu = hc$uu, zz = hc$z, pp = hc$p.sorted, word = word.counts$word[hc$p.sorted_idx])
```

```{r}
#lowest p-values 
HC %>% arrange(desc(pp)) %>% top_n(-20, pp) 

HC %>%
    top_n(-10, pp) %>%
    ggplot(aes(reorder(word, pp), pp)) + 
    geom_col(colour = 'red') + 
    ylab('p-val') +
    ggtitle("lowest p-vals")
```

```{r}
library(topicmodelstopicmode)
library(tidytext)
library(tm)
```

```{r}

DTM <- unit1.dt %>%
  cast_dtm(speech_id, word, n)

ap_lda <- LDA(DTM, k = 15, control = list(seed = 1234))
ap_lda
```

```{r}
gamma_ham <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- gamma_ham %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

```{r}
unit1 <- raw.corpus %>%
    filter(party == 'D', congress_id == 113) %>%
    select(speech_id, speech)

unit2 <- raw.corpus %>%
    filter(party == 'D', congress_id == 114) %>%
    select(speech_id, speech)
```

```{r}
library(tidytext)
library(tm)

#most interesting words from each paper
unit1.dt <- unit1 %>%
    unnest_tokens(word, speech) %>%
    mutate(word = str_extract(word, "[a-z']+")) %>%
    count(speech_id, word, sort = TRUE) %>%
    bind_tf_idf(word, speech_id, n)

unit2.dt <- unit2 %>%
    unnest_tokens(word, speech) %>%
    mutate(word = str_extract(word, "[a-z']+")) %>%
    count(speech_id, word, sort = TRUE) %>%
    bind_tf_idf(word, speech_id, n)
```

```{r}
counts1 <- unit1.dt %>% 
  arrange(desc(tf_idf)) %>%
  #filter(speech_id < 1110000050) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(speech_id) %>% 
  top_n(7) %>% 
  ungroup %>%
  count(word, sort = TRUE)

counts2 <- unit2.dt %>% 
  arrange(desc(tf_idf)) %>%
  #filter(speech_id < 1110000050) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(speech_id) %>% 
  top_n(7) %>% 
  ungroup %>%
  count(word, sort = TRUE)
```

```{r}
two.counts <- counts1 %>% 
    inner_join(counts2, by = 'word') %>%
    mutate(total.x = sum(nn.x), total.y = sum(nn.y)) %>%
    mutate(total = nn.x + nn.y) %>%
    filter(total > 10) %>%
    rowwise() %>%
        mutate(p = (nn.x+nn.y) / (total.x + total.y)) %>%
        mutate(se = sqrt(p*(1-p)*(1/total.x + 1/ total.y))) %>%
        mutate(z.score = (nn.x /total.x - nn.y / total.y) / se) %>%
        mutate(pval = 2*pnorm(-abs(z.score))) %>%
    dplyr::select(word, nn.x, nn.y, pval) 
    
hc = hc.vals(two.counts$pval, alpha = 0.5)
hc$hc.star
HC <- data_frame(uu = hc$uu, zz = hc$z, pp = hc$p.sorted, word = two.counts$word[hc$p.sorted_idx])
```

```{r}

two.counts %>%
    arrange(desc(pval)) %>%
    top_n(-20)
```

```{r}
#lowest p-values 
#HC %>% arrange(desc(pp)) %>% top_n(-20, pp) 

HC %>%
    top_n(-20, pp) %>%
    ggplot(aes(reorder(word, pp), pp)) + 
    geom_col(colour = 'red') + 
    ylab('p-val') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    ggtitle("lowest p-vals")
```

```{r}
unit1.dt %>%
  arrange(desc(tf_idf)) %>%
  filter(speech_id < 1110000100) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(speech_id) %>% 
  top_n(5) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = speech_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~speech_id, ncol = 3, scales = "free") +
  coord_flip()
```

